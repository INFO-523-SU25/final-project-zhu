[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "",
    "text": "This project proposes the development of an interpretable machine learning model for forecasting short-term volatility anomalies in the Chinese equity market, using AtHub (603881.SH) as a case study. AtHub is a data center infrastructure provider whose stock demonstrates unusually high daily volatility and frequent sensitivity to external events such as government policy announcements (Lin et al. 2024). Rather than predicting stock prices directly‚Äîa notoriously noisy and non-stationary target‚Äîthis study focuses on detecting next-day abnormal price or volume events, defined as daily returns exceeding ¬±5% or volume spikes greater than 2√ó the rolling average.\nWe aim to construct a binary classifier that leverages over 30 technical analysis (TA) indicators across momentum, volume, trend, and volatility domains. These features are engineered using the Tushare API and the tsta library, covering 218 trading days of AtHub data. The project will incorporate time-aware cross-validation to avoid look-ahead bias and SHAP analysis for post-hoc interpretability. Ensemble models like LightGBM and XGBoost will serve as the backbone of the predictive framework, selected for their robustness in handling noisy, nonlinear tabular data. The final outcome will include an interactive visualization of feature contributions, along with a brief report and presentation.\nThis proposal reflects a practical and scalable approach to market anomaly detection, especially relevant for traders and risk managers seeking data-driven early warning systems."
  },
  {
    "objectID": "proposal.html#dataset-info",
    "href": "proposal.html#dataset-info",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Dataset info",
    "text": "Dataset info\n\n\n'Dataset contains 375 rows and 31 columns.'\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 375 entries, 0 to 374\nData columns (total 31 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   ts_code            375 non-null    object \n 1   open               375 non-null    float64\n 2   high               375 non-null    float64\n 3   low                375 non-null    float64\n 4   close              375 non-null    float64\n 5   pct_chg            375 non-null    float64\n 6   vol                375 non-null    float64\n 7   amount             375 non-null    float64\n 8   volume_obv         375 non-null    float64\n 9   volume_cmf         375 non-null    float64\n 10  volume_vpt         375 non-null    float64\n 11  volume_vwap        375 non-null    float64\n 12  volume_mfi         375 non-null    float64\n 13  volatility_bbw     375 non-null    float64\n 14  volatility_atr     375 non-null    float64\n 15  volatility_ui      375 non-null    float64\n 16  trend_macd         375 non-null    float64\n 17  trend_macd_signal  375 non-null    float64\n 18  trend_macd_diff    375 non-null    float64\n 19  trend_adx          375 non-null    float64\n 20  trend_adx_pos      375 non-null    float64\n 21  trend_adx_neg      375 non-null    float64\n 22  momentum_rsi       375 non-null    float64\n 23  momentum_wr        375 non-null    float64\n 24  momentum_roc       375 non-null    float64\n 25  momentum_ao        375 non-null    float64\n 26  momentum_ppo_hist  375 non-null    float64\n 27  trend_cci          375 non-null    float64\n 28  trend_aroon_up     375 non-null    int64  \n 29  trend_aroon_down   375 non-null    int64  \n 30  trend_aroon_ind    375 non-null    int64  \ndtypes: float64(27), int64(3), object(1)\nmemory usage: 90.9+ KB"
  },
  {
    "objectID": "proposal.html#dataset-summary",
    "href": "proposal.html#dataset-summary",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Dataset Summary",
    "text": "Dataset Summary\nThe dataset is sourced via the Tushare API and engineered using Python‚Äôs tsta technical indicator library. It includes:\n\n375 daily records of AtHub stock trading from the past ~18 months\n31 columns, including:\n\nPrice data: open, high, low, close, pct_chg\nVolume metrics: vol, amount, volume_obv, volume_cmf, volume_vpt, volume_vwap, volume_mfi\nVolatility indicators: volatility_bbw, volatility_atr, volatility_ui\nTrend & momentum indicators: trend_macd, trend_adx, momentum_rsi, momentum_wr, momentum_roc, trend_aroon_up, etc.\n\n\nThis feature-rich time series provides a robust foundation for testing anomaly detection models under realistic, noisy conditions."
  },
  {
    "objectID": "proposal.html#target-anomalies",
    "href": "proposal.html#target-anomalies",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Target Anomalies",
    "text": "Target Anomalies\n\n\n\n\n\nAnomaly Class Distribution\n\n\n\nDistribution of Anomaly Labels (Target Variable)"
  },
  {
    "objectID": "proposal.html#feature-construction-strategy",
    "href": "proposal.html#feature-construction-strategy",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Feature Construction Strategy",
    "text": "Feature Construction Strategy\nTo effectively model price and volume anomalies, we engineered over 30 technical indicators across four core dimensions widely adopted in quantitative trading:\n\nMomentum (e.g., RSI, MACD, Williams %R): capture price velocity and potential reversals.\nVolume-based (e.g., OBV, MFI, VPT): track accumulation/distribution behavior.\nVolatility (e.g., ATR, Bollinger Band Width, Ulcer Index): quantify market turbulence.\nTrend strength (e.g., ADX, Aroon, CCI): detect the emergence or weakening of price trends.\n\nThese indicators were computed using the tsta Python library and merged with daily OHLCV data. We conducted correlation analysis to filter redundant signals and retain complementary ones, as illustrated in the figure below.\n\n\n\n\n\nCorrelation Between Features and Target (Anomaly)\n\n\n\n\nThis engineered feature space provides interpretable signals that are sensitive to both directional shifts and liquidity changes‚Äîtwo major components in anomaly formation."
  },
  {
    "objectID": "proposal.html#model-development-approach",
    "href": "proposal.html#model-development-approach",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Model Development Approach",
    "text": "Model Development Approach\n\nData Splitting Strategy\nGiven the time-series nature of stock data, we implement:\n\nChronological split: First 80% for training, last 20% for testing\nWalk-forward validation: Expanding window cross-validation\n\n\n\nEvaluation Metrics\nWe prioritize: - Recall: Minimizing false negatives (missed anomalies)\n\nF1-score: Balancing precision/recall\nMatthews Correlation Coefficient: Robust to class imbalance\n\n\n\nBaseline Models\n\n\n\n\n\n\n\n\nModel\nStrengths\nWeaknesses\n\n\n\n\nXGBoost\nHandles nonlinear relationships\nRequires careful tuning\n\n\nLightGBM\nEfficient with large features\nSensitive to outliers\n\n\nLogistic Regression\nInterpretable coefficients\nLimited nonlinear capacity"
  },
  {
    "objectID": "proposal.html#weekly-plan-predicting-abnormal-volatility-in-athub-603881.sh",
    "href": "proposal.html#weekly-plan-predicting-abnormal-volatility-in-athub-603881.sh",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Weekly Plan: Predicting Abnormal Volatility in AtHub (603881.SH)",
    "text": "Weekly Plan: Predicting Abnormal Volatility in AtHub (603881.SH)\n\nWeek 1: Data Collection & Exploratory Analysis (EDA)\n\nTasks:\n\nCollect 1+ year of OHLCV data for AtHub using Tushare API.\nGenerate TA features (momentum, volume, volatility, trend indicators).\nPerform EDA:\n\nVisualize price/volume trends and anomaly frequency.\nCheck for missing data, outliers, and stationarity.\nAnalyze correlation between raw price/volume metrics.\n\nDefine preliminary anomaly thresholds ($\\(5% returns, 2\\)$ volume).\n\nTools: tushare, pandas, matplotlib, ta, seaborn.\n\n\n\nWeek 2: Feature Engineering & Baseline Model\n\nTasks:\n\nRefine anomaly labels based on EDA insights.\nSplit data chronologically (e.g., 80% train, 20% test).\nTrain baseline models (XGBoost/LightGBM) and evaluate with accuracy/F1.\n\nResearch Questions Addressed:\n\nQ3 (Threshold Impact): Test initial thresholds.\n\nTools: scikit-learn, xgboost.\n\n\n\nWeek 3: Model Tuning & Interpretability\n\nTasks:\n\nOptimize hyperparameters using time-series cross-validation.\nCompare performance across thresholds (\\(\\pm\\) 3%, \\(\\pm\\) 5%, \\(\\pm\\) 7%).\nApply SHAP to identify top predictive features and patterns.\nTest feature lead times (1‚Äì3 days pre-anomaly).\n\nResearch Questions Addressed:\n\nQ1 (Predictive Horizon): Lag feature analysis.\nQ2 (Feature Importance): SHAP/partial dependence plots.\n\nTools: optuna, shap, statsmodels (Granger causality).\n\n\n\nWeek 4: Final Evaluation & Deliverables\n\nTasks:\n\nWrite-up (1,000‚Äì2,000 words):\n\nIntroduction, Methods, Results (SHAP plots, threshold analysis), Conclusion.\n\nPresentation (5 mins):\n\nQuarto slides covering motivation, methods, key findings, Q&A prep.\n\nWebsite:\n\nHost report, code, and interactive visualizations (e.g., Plotly dashboards).\n\nRepo Organization:\n\nLogical structure (e.g., data/, notebooks/, results/).\nClear index.qmd as entry point.\n\n\nTools: Quarto, plotly, pkgdown"
  },
  {
    "objectID": "proposal.html#expected-outcomes",
    "href": "proposal.html#expected-outcomes",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Expected Outcomes",
    "text": "Expected Outcomes\n\nThreshold Analysis Results:\n\nPrecision-recall curves for \\(\\pm\\) 3% vs.¬†\\(\\pm\\) 5% vs.¬†\\(\\pm\\) 7% thresholds\nOptimal threshold selection based on trading costs\n\nTop Predictive Features:\n\nSHAP summary plot of top 10 influential indicators\nTemporal importance patterns (e.g., volume leads price)\n\nPractical Trading Rules:\n\nActionable signals like:\n*‚ÄúWhen RSI &gt; 70 AND OBV &lt; 30-day average* \\(\\to\\) 67% probability of next-day drop &gt;5%‚Äù\n\nInteractive Dashboard:\n\nDynamic visualization of anomaly predictions\nThreshold adjustment interface"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "",
    "text": "This project investigates whether abnormal price and volume fluctuations in AtHub (603881.SH)‚Äîa Chinese data center infrastructure firm‚Äîcan be predicted using technical analysis (TA) features. We define volatility anomalies as daily returns exceeding ¬±5% or volume surges exceeding twice the 30-day rolling average. Drawing on over 30 engineered TA indicators spanning momentum, trend, volume, and volatility categories, we construct a supervised learning pipeline to forecast next-day anomalies. The model is evaluated using time-aware cross-validation and interpreted through SHAP analysis to reveal leading patterns and feature contributions. Results suggest that certain TA combinations (e.g., high RSI with declining OBV) consistently precede large movements, demonstrating the potential of interpretable, data-driven tools for anomaly detection in high-volatility equities."
  },
  {
    "objectID": "index.html#loading-and-initial-preparation",
    "href": "index.html#loading-and-initial-preparation",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Loading and Initial Preparation",
    "text": "Loading and Initial Preparation\n\n\nTotal observations: 375\nNumber of Columns: 31"
  },
  {
    "objectID": "index.html#target-variable-engineering",
    "href": "index.html#target-variable-engineering",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Target Variable Engineering",
    "text": "Target Variable Engineering\n\nDefine the binary target: will there be an anomaly tomorrow?\nTo better understand the imbalance in the target variable, we plot the proportion of anomaly vs.¬†normal days. An anomaly day is defined as either a \\(\\pm\\) 5% price change or a volume spike above twice the 30-day moving average. The bar chart highlights the class imbalance, a common challenge in financial anomaly detection.\n\n\n\n\n\nClass Distribution of Target Labels"
  },
  {
    "objectID": "index.html#data-cleaning",
    "href": "index.html#data-cleaning",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Data-cleaning",
    "text": "Data-cleaning\n\n\nMissing values per column:\nts_code               0\nopen                  0\nhigh                  0\nlow                   0\nclose                 0\npct_chg               0\nvol                   0\namount                0\nvolume_obv            0\nvolume_cmf            0\nvolume_vpt            0\nvolume_vwap           0\nvolume_mfi            0\nvolatility_bbw        0\nvolatility_atr        0\nvolatility_ui         0\ntrend_macd            0\ntrend_macd_signal     0\ntrend_macd_diff       0\ntrend_adx             0\ntrend_adx_pos         0\ntrend_adx_neg         0\nmomentum_rsi          0\nmomentum_wr           0\nmomentum_roc          0\nmomentum_ao           0\nmomentum_ppo_hist     0\ntrend_cci             0\ntrend_aroon_up        0\ntrend_aroon_down      0\ntrend_aroon_ind       0\nvol_ma30             29\nanomaly               0\ntarget                0\ndtype: int64"
  },
  {
    "objectID": "index.html#data-reduction",
    "href": "index.html#data-reduction",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Data Reduction",
    "text": "Data Reduction\n\nRemove unnecessary columns\n\n\nRemaining features: 30\n\n\n\n\nCorrelation Analysis\n\n\n\n\n\nCorrelation Matrix of Selected Features\n\n\n\n\n\nThere is no highly correlated features"
  },
  {
    "objectID": "index.html#data-transformation",
    "href": "index.html#data-transformation",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Data-Transformation",
    "text": "Data-Transformation\n\n\nFeature skewness before transformation:\nvol           2.260647\namount        2.817781\nvolume_obv    2.174151\nvolume_vpt    0.949351\ndtype: float64\n\n\n\nWe can see from the output, vol, amount, volume_obv is highly right skewed, and volume_vpt is a little right skewed. We can apply log transformation."
  },
  {
    "objectID": "index.html#feature-engineering",
    "href": "index.html#feature-engineering",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nCreating Lag Features\nTo capture predictive patterns leading up to volatility events, we create lagged versions of key indicators. This allows the model to detect precursor signals 1-3 days before anomalies.\n\nThese lagged features serve as candidate leading indicators, designed to capture anomaly signals up to 3 days ahead of their occurrence.\n\n\n\nCreating Rolling Statistics\n\nRolling window statistics help capture evolving market conditions and short-term trends that may precede volatility events.\n\n\n\nInteraction Features\nWe create interaction terms between key indicators that financial theory suggests may combine to signal impending volatility.\n\n\nFeature Importance\nWe use mutual information to identify the most predictive features for our anomaly target.\n\n\nTop 20 features by mutual information:\n['log_amount', 'log_vol', 'high', 'volume_vwap', 'open', 'low', 'volatility_atr_lag1', 'trend_macd', 'volatility_atr', 'log_volume_vpt_ma5', 'volatility_atr_ma10', 'volatility_atr_lag2', 'close', 'trend_cci', 'volatility_atr_lag3', 'momentum_rsi_lag2', 'volatility_ui', 'rsi_vol_interaction', 'log_volume_vpt', 'pct_chg']"
  },
  {
    "objectID": "index.html#train-test-split",
    "href": "index.html#train-test-split",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Train-Test Split",
    "text": "Train-Test Split"
  },
  {
    "objectID": "index.html#handling-class-imbalance",
    "href": "index.html#handling-class-imbalance",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Handling Class Imbalance",
    "text": "Handling Class Imbalance\nTo address the significant class imbalance (\\(\\approx\\) 15% anomalies), we implement class weighting in our models to prioritize correct identification of rare events.\n\n\nClass weights: {np.float64(0.0): np.float64(0.6118721461187214), np.float64(1.0): np.float64(2.7346938775510203)}\n\n\n\nHandling class imbalance ensures your model doesn‚Äôt ignore rare but important anomalies, which is essential for a volatility anomaly detection task."
  },
  {
    "objectID": "index.html#model-selection-and-initialization",
    "href": "index.html#model-selection-and-initialization",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Model Selection and Initialization",
    "text": "Model Selection and Initialization\nWe initialize three baseline models with class weighting to address imbalance:\n\nLogistic Regression ‚Äì interpretable linear baseline\n\nXGBoost ‚Äì robust gradient boosting\n\nLightGBM ‚Äì efficient for large feature spaces"
  },
  {
    "objectID": "index.html#model-training",
    "href": "index.html#model-training",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Model Training",
    "text": "Model Training\nWe train all models on the training set while preserving the temporal order of data.\n\n\nTraining Logistic Regression\n\n\nTraining XGBoost\nTraining LightGBM\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n[LightGBM] [Info] Number of positive: 49, number of negative: 219\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3968\n[LightGBM] [Info] Number of data points in the train set: 268, number of used features: 55\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -&gt; initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf"
  },
  {
    "objectID": "index.html#baseline-evaluation",
    "href": "index.html#baseline-evaluation",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Baseline Evaluation",
    "text": "Baseline Evaluation\nWe evaluate model performance using time-series appropriate metrics focused on anomaly detection capability.\n\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.95      0.78      0.86        54\n         1.0       0.50      0.86      0.63        14\n\n    accuracy                           0.79        68\n   macro avg       0.73      0.82      0.74        68\nweighted avg       0.86      0.79      0.81        68\n\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.87      0.89        54\n         1.0       0.56      0.64      0.60        14\n\n    accuracy                           0.82        68\n   macro avg       0.73      0.76      0.74        68\nweighted avg       0.83      0.82      0.83        68\n\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\nLightGBM Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.88      0.80      0.83        54\n         1.0       0.42      0.57      0.48        14\n\n    accuracy                           0.75        68\n   macro avg       0.65      0.68      0.66        68\nweighted avg       0.78      0.75      0.76        68\n\n\n\n\n\n\nBaseline Model Performance Comparison\n\n\n\n\n\nüß© Confusion Matrix Analysis\nThe confusion matrices above illustrate the detailed classification outcomes for each model:\n\nLogistic Regression:\n\nCorrectly identified 12 out of 14 anomalies (true positives), with only 2 false negatives.\nMisclassified 12 normal cases as anomalies (false positives), suggesting higher sensitivity but lower precision.\n\nXGBoost:\n\nAchieved a more balanced trade-off, with 9 true positives and 5 false negatives, while maintaining fewer false positives (7).\nIndicates more conservative but precise predictions.\n\nLightGBM:\n\nDetected 8 anomalies, missing 6, and misclassified 11 normal cases as anomalies.\nShows relatively weaker performance both in recall and precision.\n\n\nThese matrices reinforce the earlier observation: Logistic Regression exhibits the strongest recall, crucial for rare event detection, albeit at the cost of more false alarms.\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\nBaseline Model Performance Comparison\n\n\n\n\n\n\n\n\n\n\n\nüìä Baseline Model Performance Comparison\nTo evaluate the effectiveness of different classification models in identifying short-term volatility anomalies, we trained three baselines with class weighting to mitigate the heavy class imbalance (\\(\\approx\\) 15% anomalies):\n\nLogistic Regression\nXGBoost\nLightGBM\n\nThe bar chart above compares their performance on three key evaluation metrics:\n\nRecall (Sensitivity): Measures the model‚Äôs ability to correctly detect anomalies (true positives).\nF1-Score: Harmonic mean of precision and recall, balancing false positives and false negatives.\nMCC (Matthews Correlation Coefficient): A balanced metric even for imbalanced classes, ranging from -1 to 1.\n\n\nüîç Observations:\n\nLogistic Regression performed best across all metrics:\n\nIt achieved the highest recall (~87%), indicating strong ability to detect rare anomaly cases.\nIts F1-score (~64%) and MCC (~54%) suggest reasonably good overall balance despite the class imbalance.\n\nXGBoost delivered moderate recall (~65%) and slightly lower F1 and MCC, suggesting it is more conservative but still effective.\nLightGBM underperformed in this setup:\n\nAlthough recall was fair (~57%), its MCC dropped below 0.4, indicating weaker overall discriminative power."
  },
  {
    "objectID": "index.html#cross-validation-for-robustness-assessment",
    "href": "index.html#cross-validation-for-robustness-assessment",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Cross-Validation for Robustness Assessment",
    "text": "Cross-Validation for Robustness Assessment\nTo ensure our models generalize well and to get a more reliable estimate of performance, we implement stratified k-fold cross-validation. This approach maintains the class distribution in each fold, which is crucial given our imbalanced dataset."
  },
  {
    "objectID": "index.html#hyperparameter-tuning-for-improved-performance",
    "href": "index.html#hyperparameter-tuning-for-improved-performance",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Hyperparameter Tuning for Improved Performance",
    "text": "Hyperparameter Tuning for Improved Performance\nWe focus on tuning the Logistic Regression model since it showed the best performance in our baseline evaluation. We optimize for recall to maximize anomaly detection while balancing precision through regularization.\n\n\nFitting 5 folds for each of 28 candidates, totalling 140 fits\n\n\nGridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=LogisticRegression(class_weight='balanced',\n                                          max_iter=3000, random_state=42),\n             n_jobs=-1,\n             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                         'penalty': ['l1', 'l2'],\n                         'solver': ['liblinear', 'saga']},\n             scoring='recall', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=LogisticRegression(class_weight='balanced',\n                                          max_iter=3000, random_state=42),\n             n_jobs=-1,\n             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                         'penalty': ['l1', 'l2'],\n                         'solver': ['liblinear', 'saga']},\n             scoring='recall', verbose=1) best_estimator_: LogisticRegressionLogisticRegression(C=np.float64(0.001), class_weight='balanced', max_iter=3000,\n                   penalty='l1', random_state=42, solver='liblinear') LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=np.float64(0.001), class_weight='balanced', max_iter=3000,\n                   penalty='l1', random_state=42, solver='liblinear') \n\n\n\nWe prioritize recall, because in early warning systems, recall matters most: better to investigate a few false alerts than miss a real event."
  },
  {
    "objectID": "index.html#model-evaluation",
    "href": "index.html#model-evaluation",
    "title": "Forecasting Anomalies in AtHub‚Äôs Stock Behavior",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\n\nBest parameters: {'C': np.float64(0.001), 'penalty': 'l1', 'solver': 'liblinear'}\nBest recall score: 0.9077\n\n\nWe conducted hyperparameter tuning on the Logistic Regression model using a 5-fold stratified cross-validation strategy. The tuning process explored various combinations of regularization strength (C), penalty types (l1, l2), and solvers compatible with L1 regularization (liblinear, saga).\nBy optimizing for recall, we aimed to prioritize the detection of abnormal events (true positives), even at the potential cost of increased false positives.\nThe best-performing configuration is as follows:\n\nC: 0.001\nPenalty: L1\nSolver: liblinear\nCross-validated Recall: 0.9077\n\nThis configuration reflects a strong preference for sparsity and regularization, which is suitable for handling high-dimensional or potentially collinear feature spaces. The high recall indicates the model is effective at identifying rare but critical anomaly events.\nWe use this best estimator for final model training and evaluation.\n\n\n              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00        54\n         1.0       0.21      1.00      0.34        14\n\n    accuracy                           0.21        68\n   macro avg       0.10      0.50      0.17        68\nweighted avg       0.04      0.21      0.07        68\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model is extremely sensitive to anomalies (perfect recall), but sacrifices all specificity. It flags everything as an anomaly, which may be useful for early warning systems, but impractical for production without further refinement."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Project Overview\nThis project explores the predictive modeling of short-term volatility anomalies in the Chinese equity market, with a specific focus on the stock AtHub (603881.SH)‚Äîa data center infrastructure company. The goal is to develop a machine learning pipeline that can detect abnormal daily price or volume movements using technical analysis (TA) indicators.\nWe define anomalies as:\n\nPrice spikes or crashes: Daily returns exceeding $$5%.\nVolume surges: Daily trading volume exceeding 2√ó the 30-day moving average.\n\nBy engineering over 30 TA-based features (momentum, trend, volume, and volatility), we aim to identify leading patterns that consistently precede such events.\n\n\n\nApproach Summary\n\nData Collection: Daily OHLCV data for AtHub over a multi-year period.\nFeature Engineering: Over 30 technical indicators computed using rolling windows.\nModeling: Supervised learning for anomaly classification (binary targets).\nEvaluation: Time-aware cross-validation to preserve chronological integrity.\nInterpretation: SHAP analysis to understand key feature interactions.\n\n\n\n\nContributors\nThis project was developed by\n\nAnnabelle Zhu, Project Author\nDr.¬†Greg Chism, Professor of INFO 523."
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Project title",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Project title",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Project title",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Sun, 03 Aug 2025   Prob (F-statistic):           5.84e-08\nTime:                        11:57:20   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Project title",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Project title",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Project title",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Project title",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Project title",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g.¬†$\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Project title",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that‚Äôs what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  }
]