{
  "hash": "722a7ab441cb3980d673096c9f122ad8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Forecasting Anomalies in AtHub’s Stock Behavior\"\nsubtitle: \"Data-Driven Detection of Local Peaks and Dips\"\nauthor: \n  - name: \"Annabelle Zhu\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"This project explores the predictive modeling of short-term volatility anomalies in the Chinese equity market, with a specific focus on the stock *AtHub (603881.SH)*—a data center infrastructure company. The goal is to develop a machine learning pipeline that can detect abnormal daily price or volume movements using technical analysis (TA) indicators.\"\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    embed-resources: true\neditor: visual\nbibliography: references.bib\ncode-annotations: hover\nexecute:\n  warning: false\n  echo: false\njupyter: python3\n---\n\n# 📝 Proposal\n\nThis project proposes the development of an interpretable machine learning model for forecasting short-term volatility anomalies in the Chinese equity market, using AtHub (603881.SH) as a case study. AtHub is a data center infrastructure provider whose stock demonstrates unusually high daily volatility and frequent sensitivity to external events such as government policy announcements [@lin2024datahub]. Rather than predicting stock prices directly—a notoriously noisy and non-stationary target—this study focuses on detecting next-day abnormal price or volume events, defined as daily returns exceeding ±5% or volume spikes greater than 2× the rolling average.\n\nWe aim to construct a binary classifier that leverages over 30 technical analysis (TA) indicators across momentum, volume, trend, and volatility domains. These features are engineered using the Tushare API and the `tsta` library, covering 218 trading days of AtHub data. The project will incorporate time-aware cross-validation to avoid look-ahead bias and SHAP analysis for post-hoc interpretability. Ensemble models like LightGBM and XGBoost will serve as the backbone of the predictive framework, selected for their robustness in handling noisy, nonlinear tabular data. The final outcome will include an interactive visualization of feature contributions, along with a brief report and presentation.\n\nThis proposal reflects a practical and scalable approach to market anomaly detection, especially relevant for traders and risk managers seeking data-driven early warning systems.\n\n------------------------------------------------------------------------\n\n# 🎯 High-Level Goal\n\nTo develop a machine learning classifier that predicts next-day abnormal volatility events in AtHub (603881.SH) stock using technical analysis (TA) indicators, with anomalies defined as price movements exceeding ±5% or volume surges \\>2× the 30-day average.\n\n\n\n------------------------------------------------------------------------\n\n# 📊 Dataset\n\n::: {#cell-load-dataset .cell message='false' execution_count=2}\n\n::: {#load-dataset .cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ts_code</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>pct_chg</th>\n      <th>vol</th>\n      <th>amount</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>trend_adx_neg</th>\n      <th>momentum_rsi</th>\n      <th>momentum_wr</th>\n      <th>momentum_roc</th>\n      <th>momentum_ao</th>\n      <th>momentum_ppo_hist</th>\n      <th>trend_cci</th>\n      <th>trend_aroon_up</th>\n      <th>trend_aroon_down</th>\n      <th>trend_aroon_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>603881.SH</td>\n      <td>26.78</td>\n      <td>26.80</td>\n      <td>26.31</td>\n      <td>26.42</td>\n      <td>-1.7479</td>\n      <td>274104.38</td>\n      <td>725328.815</td>\n      <td>274104.38</td>\n      <td>-0.551020</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>-77.551020</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>603881.SH</td>\n      <td>26.81</td>\n      <td>26.95</td>\n      <td>26.50</td>\n      <td>26.89</td>\n      <td>-0.5547</td>\n      <td>328799.42</td>\n      <td>879021.084</td>\n      <td>602903.80</td>\n      <td>0.149414</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>-9.375000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.113379</td>\n      <td>66.666667</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>603881.SH</td>\n      <td>27.77</td>\n      <td>27.99</td>\n      <td>27.02</td>\n      <td>27.04</td>\n      <td>-2.8387</td>\n      <td>453556.90</td>\n      <td>1240793.683</td>\n      <td>1056460.70</td>\n      <td>-0.326345</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>-56.547619</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.214038</td>\n      <td>100.000000</td>\n      <td>8</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>603881.SH</td>\n      <td>27.81</td>\n      <td>27.98</td>\n      <td>27.38</td>\n      <td>27.83</td>\n      <td>-0.8550</td>\n      <td>438206.80</td>\n      <td>1214125.954</td>\n      <td>1494667.50</td>\n      <td>-0.084077</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>-9.523810</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.453638</td>\n      <td>94.972067</td>\n      <td>8</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>603881.SH</td>\n      <td>28.30</td>\n      <td>28.86</td>\n      <td>27.56</td>\n      <td>28.07</td>\n      <td>0.2858</td>\n      <td>694568.92</td>\n      <td>1954292.549</td>\n      <td>2189236.42</td>\n      <td>-0.125737</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>-30.980392</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.633289</td>\n      <td>107.892527</td>\n      <td>16</td>\n      <td>0</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Dataset info\n\n::: {#4d4f7f29 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n'Dataset contains 375 rows and 31 columns.'\n```\n:::\n:::\n\n\n::: {#eb603326 .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 375 entries, 0 to 374\nData columns (total 31 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   ts_code            375 non-null    object \n 1   open               375 non-null    float64\n 2   high               375 non-null    float64\n 3   low                375 non-null    float64\n 4   close              375 non-null    float64\n 5   pct_chg            375 non-null    float64\n 6   vol                375 non-null    float64\n 7   amount             375 non-null    float64\n 8   volume_obv         375 non-null    float64\n 9   volume_cmf         375 non-null    float64\n 10  volume_vpt         375 non-null    float64\n 11  volume_vwap        375 non-null    float64\n 12  volume_mfi         375 non-null    float64\n 13  volatility_bbw     375 non-null    float64\n 14  volatility_atr     375 non-null    float64\n 15  volatility_ui      375 non-null    float64\n 16  trend_macd         375 non-null    float64\n 17  trend_macd_signal  375 non-null    float64\n 18  trend_macd_diff    375 non-null    float64\n 19  trend_adx          375 non-null    float64\n 20  trend_adx_pos      375 non-null    float64\n 21  trend_adx_neg      375 non-null    float64\n 22  momentum_rsi       375 non-null    float64\n 23  momentum_wr        375 non-null    float64\n 24  momentum_roc       375 non-null    float64\n 25  momentum_ao        375 non-null    float64\n 26  momentum_ppo_hist  375 non-null    float64\n 27  trend_cci          375 non-null    float64\n 28  trend_aroon_up     375 non-null    int64  \n 29  trend_aroon_down   375 non-null    int64  \n 30  trend_aroon_ind    375 non-null    int64  \ndtypes: float64(27), int64(3), object(1)\nmemory usage: 90.9+ KB\n```\n:::\n:::\n\n\n## Dataset Summary\n\nThe dataset is sourced via the **Tushare API** and engineered using Python’s `tsta` technical indicator library. It includes:\n\n-   **375 daily records** of AtHub stock trading from the past \\~18 months\n\n-   **31 columns**, including:\n\n    -   **Price data**: `open`, `high`, `low`, `close`, `pct_chg`\n    -   **Volume metrics**: `vol`, `amount`, `volume_obv`, `volume_cmf`, `volume_vpt`, `volume_vwap`, `volume_mfi`\n    -   **Volatility indicators**: `volatility_bbw`, `volatility_atr`, `volatility_ui`\n    -   **Trend & momentum indicators**: `trend_macd`, `trend_adx`, `momentum_rsi`, `momentum_wr`, `momentum_roc`, `trend_aroon_up`, etc.\n\nThis feature-rich time series provides a robust foundation for testing anomaly detection models under realistic, noisy conditions.\n\n## Target `Anomalies`\n\n::: {#cell-target-visualization .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Anomaly Class Distribution](proposal_files/figure-html/target-visualization-output-1.png){#target-visualization}\n:::\n\nDistribution of Anomaly Labels (Target Variable)\n:::\n\n\n## Feature Construction Strategy\n\nTo effectively model price and volume anomalies, we engineered over 30 technical indicators across four core dimensions widely adopted in quantitative trading:\n\n-   **Momentum** (e.g., RSI, MACD, Williams %R): capture price velocity and potential reversals.\n-   **Volume-based** (e.g., OBV, MFI, VPT): track accumulation/distribution behavior.\n-   **Volatility** (e.g., ATR, Bollinger Band Width, Ulcer Index): quantify market turbulence.\n-   **Trend strength** (e.g., ADX, Aroon, CCI): detect the emergence or weakening of price trends.\n\nThese indicators were computed using the `tsta` Python library and merged with daily OHLCV data. We conducted correlation analysis to filter redundant signals and retain complementary ones, as illustrated in the figure below.\n\n::: {#cell-feature-target-corr .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Correlation Between Features and Target (Anomaly)](proposal_files/figure-html/feature-target-corr-output-1.png){#feature-target-corr}\n:::\n:::\n\n\nThis engineered feature space provides interpretable signals that are sensitive to both directional shifts and liquidity changes—two major components in anomaly formation.\n\n## Model Development Approach\n\n### Data Splitting Strategy\n\nGiven the time-series nature of stock data, we implement:\n\n-   **Chronological split**: First 80% for training, last 20% for testing\n\n-   **Walk-forward validation**: Expanding window cross-validation\n\n### Evaluation Metrics\n\nWe prioritize: - **Recall**: Minimizing false negatives (missed anomalies)\n\n-   **F1-score**: Balancing precision/recall\n\n-   **Matthews Correlation Coefficient**: Robust to class imbalance\n\n### Baseline Models\n\n| Model | Strengths | Weaknesses |\n|------------------------|------------------------|------------------------|\n| **XGBoost** | Handles nonlinear relationships | Requires careful tuning |\n| **LightGBM** | Efficient with large features | Sensitive to outliers |\n| **Logistic Regression** | Interpretable coefficients | Limited nonlinear capacity |\n\n------------------------------------------------------------------------\n\n# 📍 Motivation & Goals\n\n**Why AtHub (603881.SH)?** AtHub is a leading Chinese data center infrastructure provider whose stock exhibits unusually high short-term volatility, making it a strong candidate for anomaly-based forecasting. Over the past six months, its daily return volatility ($\\sigma \\approx$ 35%) has far exceeded the industry average ($\\approx$ 22%). Moreover, its price reacts sharply to regulatory announcements and policy shifts, reflecting its sensitivity to macro-level and sector-specific events.\n\nThis project aims to detect and forecast short-term abnormal volatility events in AtHub’s stock using supervised machine learning. Instead of heuristic rules, we define volatility anomalies using **quantifiable thresholds**: price changes beyond ±5% or trading volumes exceeding 2$\\times$ the 30-day average. Our key goals are:\n\n-   **Build an interpretable prediction model** using over 30 engineered technical indicators (e.g., MACD, RSI, OBV, ATR).\n-   **Evaluate event-driven prediction performance** using time-series-aware cross-validation and dynamic thresholding strategies.\n-   **Provide real-world utility** in the form of a probabilistic alert system for volatility-prone trading days.\n\nSHAP analysis is integrated to uncover feature interactions that precede volatility (e.g., “high RSI + declining OBV” may precede reversals), offering not just predictive power but also interpretability.\n\n------------------------------------------------------------------------\n\n# 👓 Research Questions\n\n-   Q1. Can TA features predict anomalies 1–3 days into the future? \n\n-   Q2. Which features drive predictions? Do they align with financial theory?\n\n-   Q3. How do anomaly thresholds ($\\pm$ 3% vs. $\\pm$ 5% vs. $\\pm$ 7% price; 1.8 $\\times$ vs. 2.5$\\times$ volume) impact model performance?\n\n------------------------------------------------------------------------\n\n# 🚩 Analysis plan\n\nHere's a refined **weekly plan** in bullet-point format that incorporates EDA before feature engineering, aligns with your research questions, and includes all required deliverables (write-up, presentation, website). Tools are listed separately for clarity:\n\n## **Weekly Plan: Predicting Abnormal Volatility in AtHub (603881.SH)**\n\n#### **Week 1: Data Collection & Exploratory Analysis (EDA)**\n\n-   **Tasks**:\n    -   Collect 1+ year of OHLCV data for AtHub using Tushare API.\n    -   Generate TA features (momentum, volume, volatility, trend indicators).\n    -   Perform EDA:\n        -   Visualize price/volume trends and anomaly frequency.\n        -   Check for missing data, outliers, and stationarity.\n        -   Analyze correlation between raw price/volume metrics.\n    -   Define preliminary anomaly thresholds (\\$\\pm$5% returns, 2$\\times\\$ volume).\n-   **Tools**: `tushare`, `pandas`, `matplotlib`, `ta`, `seaborn`.\n\n#### **Week 2: Feature Engineering & Baseline Model**\n\n-   **Tasks**:\n    -   Refine anomaly labels based on EDA insights.\n    -   Split data chronologically (e.g., 80% train, 20% test).\n    -   Train baseline models (XGBoost/LightGBM) and evaluate with accuracy/F1.\n-   **Research Questions Addressed**:\n    -   *Q3 (Threshold Impact)*: Test initial thresholds.\n-   **Tools**: `scikit-learn`, `xgboost`.\n\n#### **Week 3: Model Tuning & Interpretability**\n\n-   **Tasks**:\n    -   Optimize hyperparameters using time-series cross-validation.\n    -   Compare performance across thresholds ($\\pm$ 3%, $\\pm$ 5%, $\\pm$ 7%).\n    -   Apply SHAP to identify top predictive features and patterns.\n    -   Test feature lead times (1–3 days pre-anomaly).\n-   **Research Questions Addressed**:\n    -   *Q1 (Predictive Horizon)*: Lag feature analysis.\n    -   *Q2 (Feature Importance)*: SHAP/partial dependence plots.\n-   **Tools**: `optuna`, `shap`, `statsmodels` (Granger causality).\n\n#### **Week 4: Final Evaluation & Deliverables**\n\n-   **Tasks**:\n    -   **Write-up (1,000–2,000 words)**:\n        -   Introduction, Methods, Results (SHAP plots, threshold analysis), Conclusion.\n    -   **Presentation (5 mins)**:\n        -   Quarto slides covering motivation, methods, key findings, Q&A prep.\n    -   **Website**:\n        -   Host report, code, and interactive visualizations (e.g., Plotly dashboards).\n    -   **Repo Organization**:\n        -   Logical structure (e.g., `data/`, `notebooks/`, `results/`).\n        -   Clear `index.qmd` as entry point.\n-   **Tools**: Quarto, `plotly`, `pkgdown`\n\n## Expected Outcomes\n\n1.  **Threshold Analysis Results**:\n    -   Precision-recall curves for $\\pm$ 3% vs. $\\pm$ 5% vs. $\\pm$ 7% thresholds\n    -   Optimal threshold selection based on trading costs\n2.  **Top Predictive Features**:\n    -   SHAP summary plot of top influential indicators\n    -   Temporal importance patterns (e.g., volume leads price)\n3.  **Practical Trading Rules**:\n    -   Actionable signals like:\\\n        *\"When RSI \\> 70 AND OBV \\< 30-day average* $\\to$ 67% probability of next-day drop \\>5%\"\n4.  **Interactive Dashboard**:\n    -   Dynamic visualization of anomaly predictions\n    -   Threshold adjustment interface\n\n------------------------------------------------------------------------\n\n# 📁 Repository Organization\n\n| Folder / File Name | Description |\n|--------------------|------------------------------------|\n| `.quarto/`         | Internal Quarto system files; manages cache and config for rendering. Not manually edited. |\n| `_extra/`          | Holds supplementary files or artifacts not directly part of deliverables. |\n| `_freeze/`         | Stores frozen snapshots of document outputs to ensure reproducibility across builds. |\n| `audio/`           | Contains voice recordings for presentation. Each file corresponds to a slide in the presentation. |\n| `_site/`           | Output folder generated when the site is rendered; contains final HTML files. |\n| `data/`            | Contains all datasets used in the project, both raw and processed. Includes README for data schema and source. |\n| `images/`          | Stores all image assets, including plots and figures used in `.qmd` files. |\n| `style/`           | Contains custom theming files (e.g., `customtheming.scss`) used to style the website. |\n| `index.qmd`        | Landing page of the Quarto website; typically includes a high-level **project overview** or introduction. |\n| `about.qmd`        | Additional project background or author info. Can serve as a **detailed project description**. |\n| `proposal.qmd`     | Contains the research proposal, including motivation, methodology, timeline, and repo organization. |\n| `presentation.qmd` | A Quarto-based presentation (slides) summarizing key findings from the final report. |\n\n",
    "supporting": [
      "proposal_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}