{
  "hash": "cf75b6d4db0499321f4c00706353e1d1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Forecasting Anomalies in AtHub’s Stock Behavior\"\nsubtitle: \"INFO 523 - Final Project\"\nauthor: \n  - name: \"Annabelle Zhu\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"Project description\"\nformat:\n   html:\n    code-tools: true\n    code-overflow: wrap\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  echo: false\njupyter: python3\n---\n\n# Abstract\n\nThis project investigates whether abnormal price and volume fluctuations in AtHub (603881.SH)—a Chinese data center infrastructure firm—can be predicted using technical analysis (TA) features. We define volatility anomalies as daily returns exceeding ±5% or volume surges exceeding twice the 30-day rolling average. Drawing on over 30 engineered TA indicators spanning momentum, trend, volume, and volatility categories, we construct a supervised learning pipeline to forecast next-day anomalies. The model is evaluated using time-aware cross-validation and interpreted through SHAP analysis to reveal leading patterns and feature contributions. Results suggest that certain TA combinations (e.g., high RSI with declining OBV) consistently precede large movements, demonstrating the potential of interpretable, data-driven tools for anomaly detection in high-volatility equities.\n\n------------------------------------------------------------------------\n\n# Introduction\n\nPredicting sudden shifts in equity price or trading volume is a long-standing challenge in financial forecasting, particularly for high-volatility stocks sensitive to external shocks. This project centers on AtHub (603881.SH), a stock known for its erratic short-term behavior and policy-driven sensitivity, to assess whether machine learning models can detect early signs of abnormal market activity. Unlike traditional models that aim to forecast precise price levels, our approach reframes the task as a binary classification problem focused on identifying rare but impactful events. We rely exclusively on market-based features—technical indicators derived from historical prices and volumes—to build a predictive framework that aligns with real-world constraints where external signals (e.g., news sentiment, fundamentals) may be unavailable or delayed. By integrating explainable AI methods into the model workflow, this project also emphasizes transparency and trustworthiness in financial ML applications.\n\n------------------------------------------------------------------------\n\n# Research Questions\n\n-   Q1. Can TA features detect anomalies 1–3 days in advance? Which indicators lead?\n\n-   Q2. Which features drive predictions? Do they align with financial theory?\n\n-   Q3. How do anomaly thresholds ($\\pm$ 3% vs. $\\pm$ 5% vs. $\\pm$ 7% price; 1.8 $\\times$ vs. 2.5$\\times$ volume) impact model performance?\n\n------------------------------------------------------------------------\n\n# Exploratory Analysis\n\n## Loading and Initial Preparation\n\n\n\n\n\n::: {#89d866ad .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal observations: 375\nNumber of Columns: 31\n```\n:::\n:::\n\n\n## Target Variable Engineering\n\n### Define the binary target: will there be an anomaly tomorrow?\n\n\n\nTo better understand the imbalance in the target variable, we plot the proportion of anomaly vs. normal days. An anomaly day is defined as either a $\\pm$ 5% price change or a volume spike above twice the 30-day moving average. The bar chart highlights the class imbalance, a common challenge in financial anomaly detection.\n\n::: {#cell-target-distribution-bar .cell comment='Bar plot showing the proportion of anomaly vs. normal days in the dataset.' execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Class Distribution of Target Labels](index_files/figure-html/target-distribution-bar-output-1.png){#target-distribution-bar}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Data Prepossessing\n\n## Data-cleaning\n\n::: {#afd29504 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nMissing values per column:\nts_code               0\nopen                  0\nhigh                  0\nlow                   0\nclose                 0\npct_chg               0\nvol                   0\namount                0\nvolume_obv            0\nvolume_cmf            0\nvolume_vpt            0\nvolume_vwap           0\nvolume_mfi            0\nvolatility_bbw        0\nvolatility_atr        0\nvolatility_ui         0\ntrend_macd            0\ntrend_macd_signal     0\ntrend_macd_diff       0\ntrend_adx             0\ntrend_adx_pos         0\ntrend_adx_neg         0\nmomentum_rsi          0\nmomentum_wr           0\nmomentum_roc          0\nmomentum_ao           0\nmomentum_ppo_hist     0\ntrend_cci             0\ntrend_aroon_up        0\ntrend_aroon_down      0\ntrend_aroon_ind       0\nvol_ma30             29\nanomaly               0\ntarget                0\ndtype: int64\n```\n:::\n:::\n\n\n\n\n## Data Reduction\n\n### Remove unnecessary columns\n\n::: {#322bf232 .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nRemaining features: 30\n```\n:::\n:::\n\n\n### Correlation Analysis\n\n::: {#cell-correlation-analysis .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Correlation Matrix of Selected Features](index_files/figure-html/correlation-analysis-output-1.png){#correlation-analysis}\n:::\n:::\n\n\n> There is no highly correlated features\n\n## Data-Transformation\n\n::: {#08be4b1b .cell execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nFeature skewness before transformation:\nvol           2.260647\namount        2.817781\nvolume_obv    2.174151\nvolume_vpt    0.949351\ndtype: float64\n```\n:::\n:::\n\n\n> We can see from the output, `vol`, `amount`, `volume_obv` is highly right skewed, and `volume_vpt` is a little right skewed. We can apply log transformation.\n\n\n\n## Feature Engineering\n\n### Creating Lag Features\n\nTo capture predictive patterns leading up to volatility events, we create lagged versions of key indicators. This allows the model to detect precursor signals 1-3 days before anomalies.\n\n\n\n> These lagged features serve as candidate leading indicators, designed to capture anomaly signals up to 3 days ahead of their occurrence.\n\n### Creating Rolling Statistics\n\n\n\n> Rolling window statistics help capture evolving market conditions and short-term trends that may precede volatility events.\n\n### Interaction Features\n\nWe create interaction terms between key indicators that financial theory suggests may combine to signal impending volatility.\n\n\n\n### Feature Importance\n\n**We use mutual information to identify the most predictive features for our anomaly target.**\n\n\n\n::: {#5d52737a .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 20 features by mutual information:\n['log_amount', 'log_vol', 'high', 'volume_vwap', 'open', 'low', 'volatility_atr_lag1', 'trend_macd', 'volatility_atr', 'log_volume_vpt_ma5', 'volatility_atr_ma10', 'volatility_atr_lag2', 'close', 'trend_cci', 'volatility_atr_lag3', 'momentum_rsi_lag2', 'volatility_ui', 'rsi_vol_interaction', 'log_volume_vpt', 'pct_chg']\n```\n:::\n:::\n\n\n::: {#6f485be9 .cell execution_count=17}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Baseline Model Development\n\n## Train-Test Split\n\n\n\n## Handling Class Imbalance\n\nTo address the significant class imbalance ($\\approx$ 15% anomalies), we implement class weighting in our models to prioritize correct identification of rare events.\n\n::: {#class-weighting .cell execution_count=19}\n\n::: {.cell-output .cell-output-stdout}\n```\nClass weights: {np.float64(0.0): np.float64(0.6118721461187214), np.float64(1.0): np.float64(2.7346938775510203)}\n```\n:::\n:::\n\n\n> Handling class imbalance ensures your model doesn't ignore rare but important anomalies, which is essential for a volatility anomaly detection task.\n\n## Model Selection and Initialization\n\nWe initialize three baseline models with class weighting to address imbalance:\n\n1.  **Logistic Regression** – interpretable linear baseline\\\n2.  **XGBoost** – robust gradient boosting\\\n3.  **LightGBM** – efficient for large feature spaces\n\n\n\n## Model Training\n\nWe train all models on the training set while preserving the temporal order of data.\n\n::: {#train-models .cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining Logistic Regression\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining XGBoost\nTraining LightGBM\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n[LightGBM] [Info] Number of positive: 49, number of negative: 219\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3968\n[LightGBM] [Info] Number of data points in the train set: 268, number of used features: 55\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n```\n:::\n:::\n\n\n## Baseline Evaluation\n\nWe evaluate model performance using time-series appropriate metrics focused on anomaly detection capability.\n\n::: {#cell-evaluate-baselines .cell execution_count=22}\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.95      0.78      0.86        54\n         1.0       0.50      0.86      0.63        14\n\n    accuracy                           0.79        68\n   macro avg       0.73      0.82      0.74        68\nweighted avg       0.86      0.79      0.81        68\n\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.87      0.89        54\n         1.0       0.56      0.64      0.60        14\n\n    accuracy                           0.82        68\n   macro avg       0.73      0.76      0.74        68\nweighted avg       0.83      0.82      0.83        68\n\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\nLightGBM Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.88      0.80      0.83        54\n         1.0       0.42      0.57      0.48        14\n\n    accuracy                           0.75        68\n   macro avg       0.65      0.68      0.66        68\nweighted avg       0.78      0.75      0.76        68\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Baseline Model Performance Comparison](index_files/figure-html/evaluate-baselines-output-2.png){#evaluate-baselines}\n:::\n:::\n\n\n### 🧩 Confusion Matrix Analysis\n\nThe confusion matrices above illustrate the detailed classification outcomes for each model:\n\n* **Logistic Regression**:\n\n  * Correctly identified **12 out of 14 anomalies** (true positives), with only **2 false negatives**.\n  * Misclassified **12 normal cases** as anomalies (false positives), suggesting higher sensitivity but lower precision.\n\n* **XGBoost**:\n\n  * Achieved a more **balanced trade-off**, with **9 true positives** and **5 false negatives**, while maintaining fewer false positives (7).\n  * Indicates more conservative but precise predictions.\n\n* **LightGBM**:\n\n  * Detected **8 anomalies**, missing **6**, and misclassified **11 normal cases** as anomalies.\n  * Shows relatively weaker performance both in recall and precision.\n\nThese matrices reinforce the earlier observation: **Logistic Regression exhibits the strongest recall**, crucial for rare event detection, albeit at the cost of more false alarms.\n\n::: {#plot-results .cell execution_count=23}\n\n::: {#plot-results-1 .cell-output .cell-output-display}\n```\n<Figure size 960x576 with 0 Axes>\n```\n\nBaseline Model Performance Comparison\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/plot-results-output-2.png){#plot-results-2}\n:::\n:::\n\n\n### 📊 Baseline Model Performance Comparison\n\nTo evaluate the effectiveness of different classification models in identifying short-term volatility anomalies, we trained three baselines with class weighting to mitigate the heavy class imbalance ($\\approx$ 15% anomalies):\n\n-   **Logistic Regression**\n-   **XGBoost**\n-   **LightGBM**\n\nThe bar chart above compares their performance on three key evaluation metrics:\n\n-   **Recall** (Sensitivity): Measures the model’s ability to correctly detect anomalies (true positives).\n-   **F1-Score**: Harmonic mean of precision and recall, balancing false positives and false negatives.\n-   **MCC (Matthews Correlation Coefficient)**: A balanced metric even for imbalanced classes, ranging from -1 to 1.\n\n#### 🔍 Observations:\n\n-   **Logistic Regression** performed best across all metrics:\n\n    -   It achieved the **highest recall (\\~87%)**, indicating strong ability to detect rare anomaly cases.\n    -   Its **F1-score (\\~64%)** and **MCC (\\~54%)** suggest reasonably good overall balance despite the class imbalance.\n\n-   **XGBoost** delivered **moderate recall (\\~65%)** and slightly lower F1 and MCC, suggesting it is more conservative but still effective.\n\n-   **LightGBM** underperformed in this setup:\n\n    -   Although recall was fair (\\~57%), its MCC dropped below 0.4, indicating weaker overall discriminative power.\n\n\n---\n\n# Model Refinement\n\n\n\n## Cross-Validation for Robustness Assessment\n\nTo ensure our models generalize well and to get a more reliable estimate of performance, we implement stratified k-fold cross-validation. This approach maintains the class distribution in each fold, which is crucial given our imbalanced dataset.\n\n\n\n::: {#ea5f097f .cell execution_count=25}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){}\n:::\n:::\n\n\n## Hyperparameter Tuning for Improved Performance\n\nWe focus on tuning the Logistic Regression model since it showed the best performance in our baseline evaluation. We optimize for recall to maximize anomaly detection while balancing precision through regularization.\n\n::: {#cell-hyperparameter-tuning .cell execution_count=26}\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 28 candidates, totalling 140 fits\n```\n:::\n\n::: {#hyperparameter-tuning .cell-output .cell-output-display execution_count=26}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n                                          max_iter=3000, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;C&#x27;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;]},\n             scoring=&#x27;recall&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n                                          max_iter=3000, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;C&#x27;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;]},\n             scoring=&#x27;recall&#x27;, verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: LogisticRegression</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=np.float64(0.001), class_weight=&#x27;balanced&#x27;, max_iter=3000,\n                   penalty=&#x27;l1&#x27;, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=np.float64(0.001), class_weight=&#x27;balanced&#x27;, max_iter=3000,\n                   penalty=&#x27;l1&#x27;, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\n> We prioritize recall, because in early warning systems, recall matters most: better to investigate a few false alerts than miss a real event.\n\n## Model Evaluation\n\n::: {#a526aef7 .cell execution_count=27}\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: {'C': np.float64(0.001), 'penalty': 'l1', 'solver': 'liblinear'}\nBest recall score: 0.9077\n```\n:::\n:::\n\n\nWe conducted hyperparameter tuning on the Logistic Regression model using a 5-fold stratified cross-validation strategy. The tuning process explored various combinations of regularization strength (`C`), penalty types (`l1`, `l2`), and solvers compatible with L1 regularization (`liblinear`, `saga`).\n\nBy optimizing for **recall**, we aimed to prioritize the detection of abnormal events (true positives), even at the potential cost of increased false positives.\n\nThe best-performing configuration is as follows:\n\n* **C**: 0.001\n* **Penalty**: L1\n* **Solver**: liblinear\n* **Cross-validated Recall**: 0.9077\n\nThis configuration reflects a strong preference for sparsity and regularization, which is suitable for handling high-dimensional or potentially collinear feature spaces. The high recall indicates the model is effective at identifying rare but critical anomaly events.\n\nWe use this best estimator for final model training and evaluation.\n\n::: {#0d3f5294 .cell execution_count=28}\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00        54\n         1.0       0.21      1.00      0.34        14\n\n    accuracy                           0.21        68\n   macro avg       0.10      0.50      0.17        68\nweighted avg       0.04      0.21      0.07        68\n\n```\n:::\n:::\n\n\n::: {#44d30b25 .cell execution_count=29}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-30-output-1.png){}\n:::\n:::\n\n\n::: {#cc3a1462 .cell execution_count=30}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-31-output-1.png){}\n:::\n:::\n\n\n> The model is extremely sensitive to anomalies (perfect recall), but sacrifices all specificity. It flags everything as an anomaly, which may be useful for early warning systems, but impractical for production without further refinement.\n\n\n\n---\n\n# Model Interpretation with SHAP\n\n\nTo address our research question about which features drive predictions and whether they align with financial theory, we use SHAP (SHapley Additive exPlanations) analysis on our best-performing model.\n\n::: {#shap-analysis .cell execution_count=31}\n\n::: {.cell-output .cell-output-display}\n![SHAP Feature Importance and Dependence Plots](index_files/figure-html/shap-analysis-output-1.png){#shap-analysis-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/shap-analysis-output-2.png){#shap-analysis-2}\n:::\n:::\n\n\n1. **Feature Ranking**: \n   - `rsi_vol_interaction` (top) has the highest mean absolute SHAP value (0.0200), meaning it has the largest average impact on predictions\n   - Lagged features appear lower but still significant (e.g., `volume_cmf_lag3`)\n\n2. **Directional Impact** (from SHAP dependence plots):\n   - High `rsi_vol_interaction` $\\to$ Increases anomaly probability\n   - Low `obv_atr_interaction` $\\to$ Increases anomaly probability\n   - Extreme `macd_vol_interaction` values (both high/low) $\\to$ Raise alerts\n\n3. **Financial Theory Alignment**:\n   - Interaction terms dominate, confirming that anomalies emerge from combinations of:\n     - Overbought conditions (high RSI) + Volume spikes\n     - MACD divergence + Volatility expansion\n     - OBV breakdown + ATR surge\n\n::: {#43da1d36 .cell execution_count=32}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-33-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}