---
title: "Forecasting Anomalies in AtHub’s Stock Behavior"
subtitle: "INFO 523 - Final Project"
author: 
  - name: "Annabelle Zhu"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

# Abstract

This project investigates whether abnormal price and volume fluctuations in AtHub (603881.SH)—a Chinese data center infrastructure firm—can be predicted using technical analysis (TA) features. We define volatility anomalies as daily returns exceeding ±5% or volume surges exceeding twice the 30-day rolling average. Drawing on over 30 engineered TA indicators spanning momentum, trend, volume, and volatility categories, we construct a supervised learning pipeline to forecast next-day anomalies. The model is evaluated using time-aware cross-validation and interpreted through SHAP analysis to reveal leading patterns and feature contributions. Results suggest that certain TA combinations (e.g., high RSI with declining OBV) consistently precede large movements, demonstrating the potential of interpretable, data-driven tools for anomaly detection in high-volatility equities.

------------------------------------------------------------------------

# Introduction

Predicting sudden shifts in equity price or trading volume is a long-standing challenge in financial forecasting, particularly for high-volatility stocks sensitive to external shocks. This project centers on AtHub (603881.SH), a stock known for its erratic short-term behavior and policy-driven sensitivity, to assess whether machine learning models can detect early signs of abnormal market activity. Unlike traditional models that aim to forecast precise price levels, our approach reframes the task as a binary classification problem focused on identifying rare but impactful events. We rely exclusively on market-based features—technical indicators derived from historical prices and volumes—to build a predictive framework that aligns with real-world constraints where external signals (e.g., news sentiment, fundamentals) may be unavailable or delayed. By integrating explainable AI methods into the model workflow, this project also emphasizes transparency and trustworthiness in financial ML applications.

------------------------------------------------------------------------

# Research Questions

-   Q1. Can TA features detect anomalies 1–3 days in advance? Which indicators lead?

-   Q2. Which features drive predictions? Do they align with financial theory?

-   Q3. How do anomaly thresholds ($\pm$ 3% vs. $\pm$ 5% vs. $\pm$ 7% price; 1.8 $\times$ vs. 2.5$\times$ volume) impact model performance?

------------------------------------------------------------------------

# Exploratory Analysis

## Loading and Initial Preparation

```{python}
#| label: load packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
#| label: load dataset
df = pd.read_csv("data/stock_cleaned.csv")
```

```{python}
# Display basic info
print(f"Total observations: {len(df)}")
print(f"Number of Columns: {len(df.columns)}")
```

## Target Variable Engineering

### Define the binary target: will there be an anomaly tomorrow?

```{python}
# comment: Calculate 30-day volume moving average and find ± pct_chg 
df['vol_ma30'] = df['vol'].rolling(30).mean()

df['anomaly'] = ((df['pct_chg'].abs() >= 5) | 
                (df['vol'] > 2 * df['vol'].rolling(30).mean())).astype(int)
                
df['target'] = df['anomaly'].shift(-1)
# Remove last row (no future data)
df = df.iloc[:-1] # drop last row with NaN target

```

To better understand the imbalance in the target variable, we plot the proportion of anomaly vs. normal days. An anomaly day is defined as either a $\pm$ 5% price change or a volume spike above twice the 30-day moving average. The bar chart highlights the class imbalance, a common challenge in financial anomaly detection.

```{python}
#| label: target-distribution-bar
#| fig-cap: "Class Distribution of Target Labels"
#| comment: "Bar plot showing the proportion of anomaly vs. normal days in the dataset."

# Compute normalized class distribution of anomaly labels
target_dist = df['anomaly'].value_counts(normalize=True)

# Plot class distribution as a bar chart
plt.figure(figsize=(8, 5))
sns.barplot(x=target_dist.index, y=target_dist.values, palette=['#66c2a5', '#fc8d62'])
plt.title('Target Class Distribution')
plt.xlabel('Is Anomaly Day?')
plt.ylabel('Proportion')
plt.xticks([0, 1], ['Normal', 'Anomaly'])
plt.show()

```

### Data-cleaning

```{python}
# Check for missing values
print("Missing values per column:")
print(df.isnull().sum())

```

```{python}
# Handle missing values
df = df.dropna(subset=['vol_ma30'])  # Remove rows without volume MA
df = df.fillna(method='ffill')  # Forward fill other missing values

```

### Data Reduction

#### Remove unnecessary columns

```{python}
to_drop = ['ts_code', 'trend_adx_pos', 'trend_adx_neg', 'trend_aroon_ind']
df = df.drop(columns=to_drop)
print(f"Remaining features: {df.shape[1]}")
```

#### Correlation Analysis

```{python}
#| label: correlation-analysis
#| fig-cap: "Correlation Matrix of Selected Features"
# Calculate correlation matrix
corr_matrix = df.corr().abs()

# Visualize correlation matrix
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix[['anomaly']].sort_values(
  'anomaly'
), annot=True, center=0, cmap=sns.diverging_palette(220, 10, as_cmap=True),)
plt.title('Feature Correlation Matrix')
plt.show()

```

> There is no highly correlated features

### Data-Transformation

```{python}
# Identify skewed features
skewed_features = ['vol', 'amount', 'volume_obv', 'volume_vpt']
print("Feature skewness before transformation:")
print(df[skewed_features].skew())
```

> We can see from the output, `vol`, `amount`, `volume_obv` is highly right skewed, and `volume_vpt` is a little right skewed. We can apply log transformation.

```{python}
for feature in skewed_features:
    df[feature] = df[feature].clip(lower=0)
    df[f'log_{feature}'] = np.log1p(df[feature])

df_scaled = df.drop(columns=skewed_features).copy()

```

### Feature Engineering

#### Creating Lag Features

To capture predictive patterns leading up to volatility events, we create lagged versions of key indicators. This allows the model to detect precursor signals 1-3 days before anomalies.

```{python}
#| label: create-lag-features
# Create lag features for key indicators
lags = [1, 2, 3]
features_to_lag = [
    'volatility_atr', 'log_volume_vpt', 'trend_macd_diff', 
    'momentum_rsi', 'log_volume_obv', 'volume_cmf'
]


for feature in features_to_lag:
    for lag in lags:
        df_scaled[f'{feature}_lag{lag}'] = df_scaled[feature].shift(lag)


```




> These lagged features serve as candidate leading indicators, designed to capture anomaly signals up to 3 days ahead of their occurrence.



#### Creating Rolling Statistics


```{python}
#| label: rolling-features
# Calculate rolling statistics
windows = [5, 10]
for window in windows:
    df_scaled.loc[:, f'log_volume_vpt_ma{window}'] = df_scaled['log_volume_vpt'].rolling(window).mean()
    df_scaled.loc[:, f'momentum_rsi_ma{window}'] = df_scaled['momentum_rsi'].rolling(window).mean()
    df_scaled.loc[:, f'volatility_atr_ma{window}'] = df_scaled['volatility_atr'].rolling(window).mean()

# Create volatility spike indicator
df_scaled.loc[:, 'volatility_spike'] = (
    df_scaled['volatility_atr'] > 1.5 * df_scaled['volatility_atr_ma5']
).astype(int)

# Drop rows with missing values from rolling operations
df_scaled = df_scaled.dropna()


```

> Rolling window statistics help capture evolving market conditions and short-term trends that may precede volatility events.

#### Interaction Features

We create interaction terms between key indicators that financial theory suggests may combine to signal impending volatility.

```{python}
# Create interaction features
df_scaled['rsi_vol_interaction'] = df_scaled['momentum_rsi'] * df_scaled['log_vol']
df_scaled['macd_vol_interaction'] = df_scaled['trend_macd_diff'] * df_scaled['log_vol']
df_scaled['obv_atr_interaction'] = df_scaled['log_volume_obv'] * df_scaled['volatility_atr']

```

### Feature Importance

**We use mutual information to identify the most predictive features for our anomaly target.**

```{python}
#| label: mutual-information
#| fig-cap: "Top Features by Mutual Information with Anomaly Target"
from sklearn.feature_selection import mutual_info_classif

# Calculate mutual information
X = df_scaled.drop(columns=['anomaly', 'vol_ma30', 'target'])
y = df_scaled['target']
mi_scores = mutual_info_classif(X, y, random_state=42)
mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)

```

```{python}
# Select top 20 features
top_features = mi_series.head(20).index.tolist()
print(f"Top 20 features by mutual information:\n{top_features}")

```

```{python}
# Plot feature importance
plt.figure(figsize=(10, 8))
mi_series.head(20).sort_values().plot(kind='barh', color='teal')
plt.title('Top 20 Features by Mutual Information')
plt.xlabel('Mutual Information Score')
plt.ylabel('Features')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

```

------------------------------------------------------------------------

# Data Prepossessing







